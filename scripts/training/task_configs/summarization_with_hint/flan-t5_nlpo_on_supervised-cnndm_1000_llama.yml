tokenizer:
  model_name: ./sft4lms/ckpt/cnndm_1000/textrank-all/flan-t5-base-ep5/best_checkpoint/
  padding_side: "right"
  truncation_side: "right"
  pad_token_as_eos_token: False

llm:
  use: True
  model: '/public11_data/zs/zs/llama2_model/7b/'
  hf_access_token: "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"

reward_fn:
  id: summarization_with_hint_llama
  args:
    temperature: 0.7
    max_tokens: 160
    num_seqs: 4
    selection_strategy: "choose_all"
    top_p: 1.0
    stop_words: ["Article:", "Q:",  "A:", "<|im_end|>"]
    prompt_prefix: "Extract the keywords: "
    prompt_path: "./prompts/cnndm_fs.txt"
    hint_prompt_path: "./prompts/cnndm_hint_fs.txt"
    gpt3_metrics: ["bert_score", "meteor"]
    use_baseline: False
    t5_coef: 0.
    t5_metric: "hint_hit"
    t5_pos_coef: 1.0
    t5_neg_coef: -0.2 # -0.1
    step_reward_coef: 1.
    split_token: ";"
    split_token_id: 117 # token id of t5 for ";"


env:
  n_envs: 1
  args:
    max_prompt_length: 512
    max_episode_length: 100
    terminate_on_eos: True
    prompt_truncation_side: "right"
    context_start_token: 0

datapool:
  id: cnn_daily_mail_with_hint

  args:
    prompt_prefix: "Extract the keywords: "
    dataset: "cnndm"
    n_train: 1000 # 原本是1000
    n_val: 500 # 原本是500
    n_test: 500 # 原本是500
    extraction_mode: "textrank"
    extraction_source: "all"

alg:
  id: nlpo
  args:
    n_steps: 512 # 512
    batch_size: 10 #设置成10刚好能跑
    verbose: 1
    learning_rate: 0.000002
    n_epochs: 5 # 5
    ent_coef: 0.0
    vf_coef: 0.5
  kl_div:
    coeff: 0.005 # 0.005
    target_kl: 0.5 # 0.5
  policy:
    id: maskable_seq2seq_lm_actor_critic_policy
    args:
      model_name: ./sft4lms/ckpt/cnndm_1000/textrank-all/flan-t5-base-ep5/best_checkpoint/
      apply_model_parallel: False
      prompt_truncation_side: "right"
      min_tokens_to_keep: 100
      top_mask: 0.9
      mask_type: "learned_top_p"
      target_update_iterations: 20
      generation_kwargs:
        min_length: 10
        max_new_tokens: 80
        do_sample: True
        top_k: 100


train_evaluation:
  eval_batch_size: 10
  n_iters: 20
  eval_every: 2
  save_every: 2
  metrics:
    - id: summarization_with_hint_llama
      args:
        split_token: ";"
        split_token_id: 117 # token id of t5 for ";"
        temperature: 0.7
        max_tokens: 160
        num_seqs: 3
        selection_strategy: "choose_all"
        top_p: 1.0
        stop_words: ["Article:", "Q:",  "A:", "<|im_end|>"]
        prompt_prefix: "Extract the keywords: "
        prompt_path: "./prompts/cnndm_fs.txt"
        hint_prompt_path: "./prompts/cnndm_hint_fs.txt"
        evaluate_policy_model: True
        use_llm: True
        gpt3_metrics:
          - id: meteor
            args: {}
          - id: rouge
            args:
              use_single_ref: False
          - id: bleu
            args: {}
          - id: bert_score
            args:
              language: en
          # # - id: bleurt
          #   args:
          #     config_name: bleurt-large-512
        t5_metrics:
          - id: "hint_hit"
            args:
              split: ";"
  generation_kwargs:
    min_length: 10
    max_new_tokens: 80 # 原本是80
    do_sample: True
    top_k: 0
    temperature: 0.7

